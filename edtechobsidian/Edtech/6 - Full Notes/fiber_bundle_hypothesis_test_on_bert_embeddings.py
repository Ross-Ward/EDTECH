# -*- coding: utf-8 -*-
"""Fiber Bundle Hypothesis Test on BERT Embeddings

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W82fMkiROr4fCXu3uDgGBKWFaBbSm5Yw

**Notebook Description: Fiber Bundle Hypothesis Test on BERT Embeddings**

This Python notebook implements a framework to test the "fiber bundle hypothesis" on token embeddings derived from a pre-trained BERT model, based on the methodology described in the research paper "Token Embeddings Violate the Manifold Hypothesis"[cite: 1].

**1. Setup and Dependencies:**
* It imports necessary libraries: `numpy` for numerical operations, `torch` and `transformers` (specifically `BertTokenizer`, `BertModel`) for loading and using the BERT model, `scipy.spatial.distance` for calculating distances, `scipy.stats.ttest_ind` for statistical tests, `statsmodels.stats.multitest` for p-value correction, and `matplotlib.pyplot` for plotting.

**2. Data Generation: BERT Embeddings:**
* It loads the pre-trained 'bert-base-uncased' tokenizer and model.
* A predefined list of 50 sentences is provided, containing multiple instances of the words "bank", "river", and "code" in different contexts.
* The `get_bert_embeddings` function processes each sentence:
    * It tokenizes the sentence using the BERT tokenizer.
    * It feeds the tokens through the BERT model to get the last hidden states (embeddings).
    * It specifically extracts the embedding vector corresponding to the *first occurrence* of the target word ("bank", "river", or "code") in each sentence.
* The result is `E_vecs`, a NumPy array where each row is the contextual embedding of a target token.

**3. Core Methodology Functions:**
* **`compute_distances`**: Calculates and sorts the Euclidean distances from one token embedding to all others.
* **`compute_nx_r`**: Counts how many tokens fall within specific radius values (`r_values`) from a given token.
* **`estimate_slopes`**: Calculates the local slope of the log-log plot (log N(r) vs. log r) using a three-point centered difference method. This estimates the local dimension.
* **`detect_slope_changes`**: Implements a simplified method to find significant *increases* in the slope. It uses a sliding window approach:
    * For each point, it compares the slopes in a window *before* the point to the slopes in a window *after* the point using an independent samples t-test (`ttest_ind`).
    * If the p-value is below a threshold (`alpha`) *and* the mean slope after the point is greater than the mean slope before, it flags a significant increasing change point. This corresponds to a rejection of the fiber bundle hypothesis according to the paper's criteria[cite: 138, 143].
* **`test_fiber_bundle_hypothesis`**: This main function orchestrates the process for each token:
    * Generates logarithmically spaced radius values (`r_values`).
    * Calls the functions above to compute distances, N(r), log-log data, and slopes.
    * Detects slope changes using `detect_slope_changes`.
    * Determines whether to "Reject" or "Fail to Reject" the hypothesis based on detected slope increases.
    * Collects all raw p-values from the t-tests for later correction.
    * **Dimension Estimation:** If a rejection occurs, it attempts to estimate 'Base' and 'Fiber' dimensions by taking the mean slope *before* the first significant change point (Base) and the mean slope within the window *starting at* the change point (Fiber). This is a specific implementation choice for interpreting the slopes discussed in the paper[cite: 160].
    * **Visualization:** For the first few tokens, it generates plots showing the log-log curve and the slope curve, marking detected change points.
    * **Multiple Testing:** It applies the Holm-Bonferroni correction to all collected p-values using `multipletests`, though the rejection decisions in this specific code are made *before* this correction based on the raw p-values.

**4. Execution and Output:**
* The `test_fiber_bundle_hypothesis` function is called with the generated BERT embeddings.
* It prints the test result ("Reject" / "Fail to Reject") for each token, along with its original word and some context.
* It prints the estimated Base and Fiber dimensions for each token where applicable.
* Finally, it prints a summary count of how many tokens resulted in a rejection of the fiber bundle hypothesis.

In essence, the notebook provides a practical, albeit simplified, implementation of the fiber bundle test, applying it to real contextual embeddings from BERT to identify tokens whose local geometric structure (as estimated by slope changes) violates the conditions of a fiber bundle.
"""

import numpy as np
from scipy.spatial import distance
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Simulated token embeddings (replace with actual LLM embeddings)
np.random.seed(42)
n_tokens = 100  # Number of tokens
embedding_dim = 512  # Embedding dimension (e.g., typical for LLMs)
E_vecs = np.random.randn(n_tokens, embedding_dim)  # Random embeddings for demonstration

def compute_distances(embeddings, token_idx):
    """Step 3: Compute Euclidean distances from token_idx to all other tokens."""
    x = embeddings[token_idx]
    distances = distance.cdist([x], embeddings, 'euclidean')[0]
    return np.sort(distances)  # Step 4: Sort distances in ascending order

def compute_nx_r(distances, r_values):
    """Step 4: Compute N_x(r) - number of tokens within radius r."""
    return np.array([np.sum(distances <= r) for r in r_values])

def estimate_slopes(log_r, log_nx_r):
    """Step 6: Estimate slopes using three-point centered difference method."""
    slopes = np.zeros(len(log_r))
    for i in range(1, len(log_r) - 1):
        slopes[i] = (log_nx_r[i + 1] - log_nx_r[i - 1]) / (log_r[i + 1] - log_r[i - 1])
    # Extrapolate edges (simplified)
    slopes[0] = slopes[1]
    slopes[-1] = slopes[-2]
    return slopes

def detect_slope_changes(slopes, window_size=5, alpha=1e-3):
    """Step 7: Detect significant slope changes (simplified CFAR-like approach)."""
    changes = []
    p_values = []
    for i in range(window_size, len(slopes) - window_size):
        before = slopes[i - window_size:i]
        after = slopes[i:i + window_size]
        t_stat, p_val = ttest_ind(before, after, equal_var=False)
        if p_val < alpha and np.mean(after) > np.mean(before):  # Slope increase
            changes.append(i)
            p_values.append(p_val)
    return changes, p_values

def test_fiber_bundle_hypothesis(embeddings, r_min=0.1, r_max=10.0, n_r=100, alpha=1e-3):
    """Main function to test fiber bundle hypothesis for all tokens."""
    r_values = np.linspace(r_min, r_max, n_r)  # Step 5: Range of radius values
    log_r = np.log(r_values)
    results = []
    all_p_values = []

    for token_idx in range(len(embeddings)):
        # Step 3-4: Compute and sort distances
        distances = compute_distances(embeddings, token_idx)

        # Step 5: Compute N_x(r) and prepare log-log data
        nx_r = compute_nx_r(distances, r_values)
        log_nx_r = np.log(nx_r + 1e-10)  # Avoid log(0)

        # Step 6: Estimate slopes
        slopes = estimate_slopes(log_r, log_nx_r)

        # Step 7: Detect slope changes
        changes, p_vals = detect_slope_changes(slopes, alpha=alpha)
        all_p_values.extend(p_vals)

        # Step 8: Hypothesis testing
        if changes:  # If significant slope increases are detected
            results.append((token_idx, "Reject"))
        else:
            results.append((token_idx, "Fail to Reject"))

    # Step 9: Multiple test correction (Holm-Bonferroni)
    if all_p_values:
        corrected = multipletests(all_p_values, alpha=alpha, method='holm')[1]
        # Update results based on corrected p-values (simplified here)

    # Step 10: Output results
    return results

# Run the framework
results = test_fiber_bundle_hypothesis(E_vecs)

# Print results
for token_idx, result in results:
    print(f"Token {token_idx}: {result}")

# Optional: Aggregate results
rejections = sum(1 for _, r in results if r == "Reject")
print(f"Total tokens where fiber bundle hypothesis is rejected: {rejections}/{n_tokens}")

import numpy as np
import torch
from transformers import BertTokenizer, BertModel
from scipy.spatial import distance
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests
import matplotlib.pyplot as plt

# Load BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
model.eval()

# Larger, diverse set of sentences
sentences = [
    "The bank near the river was flooded.", "She deposited money in the bank account.",
    "The river flowed through the valley.", "He wrote code for the new software.",
    "The secret code unlocked the door.", "The fish swam in the deep river.",
    "The bank offered a loan to the customer.", "She debugged the code all night.",
    "The river bank was covered in grass.", "The code ran perfectly on the server.",
    "A tree grew by the river side.", "The bank teller smiled at her.",
    "He cracked the code in an hour.", "The river current was strong.",
    "The bank vault was secure.", "She optimized the code for speed.",
    "The code breaker was a genius.", "The river delta spread wide.",
    "The bank branch opened early.", "The code update fixed the bug.",
    "The dog ran along the river path.", "The bank charged a high fee.",
    "She learned to code in Python.", "The river water was cold.",
    "The bank building was tall.", "The code failed to compile.",
    "The river bridge was old.", "The bank manager was strict.",
    "He shared the code online.", "The river flooded the town.",
    "The bank safe was locked.", "The code executed quickly.",
    "The river bend was sharp.", "The bank loan was approved.",
    "She tested the code thoroughly.", "The river shore was sandy.",
    "The bank staff was friendly.", "The code error was subtle.",
    "The river flowed quietly.", "The bank window was open.",
    "He reviewed the code changes.", "The river depth was measured.",
    "The bank rate increased.", "The code logic was complex.",
    "The river carried debris.", "The bank app was updated.",
    "She wrote clean code daily.", "The river edge was steep.",
    "The bank policy was clear.", "The code version was new."
]
target_tokens = ["bank", "bank", "river", "code", "code", "river", "bank", "code", "bank", "code",
                 "river", "bank", "code", "river", "bank", "code", "code", "river", "bank", "code",
                 "river", "bank", "code", "river", "bank", "code", "river", "bank", "code", "river",
                 "bank", "code", "river", "bank", "code", "river", "bank", "code", "river", "bank",
                 "code", "river", "bank", "code", "river", "bank", "code", "river", "bank", "code"]
n_tokens = len(target_tokens)  # 50 tokens

# Get BERT embeddings
def get_bert_embeddings(sentences, target_tokens):
    embeddings = []
    for sentence, target in zip(sentences, target_tokens):
        inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)
        with torch.no_grad():
            outputs = model(**inputs)
        hidden_states = outputs.last_hidden_state[0].numpy()
        tokens = tokenizer.tokenize(sentence)
        target_idx = tokens.index(target)
        embedding = hidden_states[target_idx + 1]  # Skip [CLS]
        embeddings.append(embedding)
    return np.array(embeddings)

E_vecs = get_bert_embeddings(sentences, target_tokens)

def compute_distances(embeddings, token_idx):
    x = embeddings[token_idx]
    distances = distance.cdist([x], embeddings, 'euclidean')[0]
    return np.sort(distances)

def compute_nx_r(distances, r_values):
    return np.array([np.sum(distances <= r) for r in r_values])

def estimate_slopes(log_r, log_nx_r):
    slopes = np.zeros(len(log_r))
    for i in range(1, len(log_r) - 1):
        slopes[i] = (log_nx_r[i + 1] - log_nx_r[i - 1]) / (log_r[i + 1] - log_r[i - 1])
    slopes[0] = slopes[1]
    slopes[-1] = slopes[-2]
    return slopes

def detect_slope_changes(slopes, window_size=10, alpha=0.05):
    changes = []
    p_values = []
    for i in range(window_size, len(slopes) - window_size):
        before = slopes[i - window_size:i]
        after = slopes[i:i + window_size]
        t_stat, p_val = ttest_ind(before, after, equal_var=False)
        if p_val < alpha and np.mean(after) > np.mean(before):
            changes.append(i)
            p_values.append(p_val)
    return changes, p_values

def test_fiber_bundle_hypothesis(embeddings, r_min=0.01, r_max=20.0, n_r=200, alpha=0.05):
    r_values = np.linspace(r_min, r_max, n_r)
    log_r = np.log(r_values)
    results = []
    all_p_values = []

    for token_idx in range(len(embeddings)):
        distances = compute_distances(embeddings, token_idx)
        nx_r = compute_nx_r(distances, r_values)
        log_nx_r = np.log(nx_r + 1e-10)

        slopes = estimate_slopes(log_r, log_nx_r)
        changes, p_vals = detect_slope_changes(slopes, alpha=alpha)
        all_p_values.extend(p_vals)

        if changes:
            results.append((token_idx, "Reject"))
        else:
            results.append((token_idx, "Fail to Reject"))

        # Diagnostics and visualization for first few tokens
        if token_idx < 3:
            print(f"Token {token_idx} distances: {distances[:5]}... (min: {distances.min()}, max: {distances.max()})")
            print(f"Token {token_idx} slope stats: min={slopes.min():.2f}, max={slopes.max():.2f}, mean={slopes.mean():.2f}")
            plt.figure(figsize=(12, 5))
            plt.subplot(1, 2, 1)
            plt.plot(log_r, log_nx_r, label="log N_x(r) vs log r")
            plt.xlabel("log r")
            plt.ylabel("log N_x(r)")
            plt.title(f"Log-Log Plot (Token {token_idx})")
            plt.legend()

            plt.subplot(1, 2, 2)
            plt.plot(log_r, slopes, label="Slopes")
            for change in changes:
                plt.axvline(log_r[change], color='r', linestyle='--')
            plt.xlabel("log r")
            plt.ylabel("Slope")
            plt.title(f"Slopes with Changes (Token {token_idx})")
            plt.legend()
            plt.tight_layout()
            plt.show()

    if all_p_values:
        corrected = multipletests(all_p_values, alpha=alpha, method='holm')[1]

    return results

# Run the test
results = test_fiber_bundle_hypothesis(E_vecs)

# Print results with context
labels = [f"{tok} ({ctx.split()[1] if len(ctx.split()) > 1 else ctx})" for tok, ctx in zip(target_tokens, sentences)]
for (token_idx, result), label in zip(results, labels):
    print(f"Token '{label}' (idx {token_idx}): {result}")

rejections = sum(1 for _, r in results if r == "Reject")
print(f"Total tokens where fiber bundle hypothesis is rejected: {rejections}/{n_tokens}")

import numpy as np
import torch
from transformers import BertTokenizer, BertModel
from scipy.spatial import distance
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests
import matplotlib.pyplot as plt

# Load BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
model.eval()

# Dataset: 50 sentences with target tokens
sentences = [
    "The bank near the river was flooded.", "She deposited money in the bank account.",
    "The river flowed through the valley.", "He wrote code for the new software.",
    "The secret code unlocked the door.", "The fish swam in the deep river.",
    "The bank offered a loan to the customer.", "She debugged the code all night.",
    "The river bank was covered in grass.", "The code ran perfectly on the server.",
    "A tree grew by the river side.", "The bank teller smiled at her.",
    "He cracked the code in an hour.", "The river current was strong.",
    "The bank vault was secure.", "She optimized the code for speed.",
    "The code breaker was a genius.", "The river delta spread wide.",
    "The bank branch opened early.", "The code update fixed the bug.",
    "The dog ran along the river path.", "The bank charged a high fee.",
    "She learned to code in Python.", "The river water was cold.",
    "The bank building was tall.", "The code failed to compile.",
    "The river bridge was old.", "The bank manager was strict.",
    "He shared the code online.", "The river flooded the town.",
    "The bank safe was locked.", "The code executed quickly.",
    "The river bend was sharp.", "The bank loan was approved.",
    "She tested the code thoroughly.", "The river shore was sandy.",
    "The bank staff was friendly.", "The code error was subtle.",
    "The river flowed quietly.", "The bank window was open.",
    "He reviewed the code changes.", "The river depth was measured.",
    "The bank rate increased.", "The code logic was complex.",
    "The river carried debris.", "The bank app was updated.",
    "She wrote clean code daily.", "The river edge was steep.",
    "The bank policy was clear.", "The code version was new."
]
target_tokens = ["bank", "bank", "river", "code", "code", "river", "bank", "code", "bank", "code",
                 "river", "bank", "code", "river", "bank", "code", "code", "river", "bank", "code",
                 "river", "bank", "code", "river", "bank", "code", "river", "bank", "code", "river",
                 "bank", "code", "river", "bank", "code", "river", "bank", "code", "river", "bank",
                 "code", "river", "bank", "code", "river", "bank", "code", "river", "bank", "code"]
n_tokens = len(target_tokens)  # 50 tokens

# Get BERT embeddings
def get_bert_embeddings(sentences, target_tokens):
    embeddings = []
    for sentence, target in zip(sentences, target_tokens):
        inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)
        with torch.no_grad():
            outputs = model(**inputs)
        hidden_states = outputs.last_hidden_state[0].numpy()
        tokens = tokenizer.tokenize(sentence)
        target_idx = tokens.index(target)
        embedding = hidden_states[target_idx + 1]  # Skip [CLS]
        embeddings.append(embedding)
    return np.array(embeddings)

E_vecs = get_bert_embeddings(sentences, target_tokens)

def compute_distances(embeddings, token_idx):
    x = embeddings[token_idx]
    distances = distance.cdist([x], embeddings, 'euclidean')[0]
    return np.sort(distances)

def compute_nx_r(distances, r_values):
    return np.array([np.sum(distances <= r) for r in r_values])

def estimate_slopes(log_r, log_nx_r):
    slopes = np.zeros(len(log_r))
    for i in range(1, len(log_r) - 1):
        slopes[i] = (log_nx_r[i + 1] - log_nx_r[i - 1]) / (log_r[i + 1] - log_r[i - 1])
    slopes[0] = slopes[1]
    slopes[-1] = slopes[-2]
    return slopes

def detect_slope_changes(slopes, window_size=10, alpha=0.05):
    changes = []
    p_values = []
    for i in range(window_size, len(slopes) - window_size):
        before = slopes[i - window_size:i]
        after = slopes[i:i + window_size]
        t_stat, p_val = ttest_ind(before, after, equal_var=False)
        if p_val < alpha and np.mean(after) > np.mean(before):
            changes.append(i)
            p_values.append(p_val)
    return changes, p_values

def test_fiber_bundle_hypothesis(embeddings, r_min=0.01, r_max=20.0, n_r=200, alpha=0.05, window_size=10):
    r_values = np.linspace(r_min, r_max, n_r)
    log_r = np.log(r_values)
    results = []
    all_p_values = []
    dimensions = []  # Store base and fiber dimensions

    for token_idx in range(len(embeddings)):
        distances = compute_distances(embeddings, token_idx)
        nx_r = compute_nx_r(distances, r_values)
        log_nx_r = np.log(nx_r + 1e-10)

        slopes = estimate_slopes(log_r, log_nx_r)
        changes, p_vals = detect_slope_changes(slopes, window_size=window_size, alpha=alpha)
        all_p_values.extend(p_vals)

        # Extract base and fiber dimensions if a change is detected
        if changes:
            results.append((token_idx, "Reject"))
            change_idx = changes[0]  # Use the first detected change
            base_dim = np.mean(slopes[:change_idx]) if change_idx > 0 else 0
            fiber_dim = np.mean(slopes[change_idx:change_idx + window_size])
            dimensions.append((base_dim, fiber_dim))
            if token_idx < 3:
                print(f"Token {token_idx} dimensions: Base={base_dim:.2f}, Fiber={fiber_dim:.2f}")
        else:
            results.append((token_idx, "Fail to Reject"))
            dimensions.append((None, None))
            if token_idx < 3:
                print(f"Token {token_idx} dimensions: No significant change detected")

        # Diagnostics and visualization for first few tokens
        if token_idx < 3:
            print(f"Token {token_idx} distances: {distances[:5]}... (min: {distances.min()}, max: {distances.max()})")
            print(f"Token {token_idx} slope stats: min={slopes.min():.2f}, max={slopes.max():.2f}, mean={slopes.mean():.2f}")
            plt.figure(figsize=(12, 5))
            plt.subplot(1, 2, 1)
            plt.plot(log_r, log_nx_r, label="log N_x(r) vs log r")
            plt.xlabel("log r")
            plt.ylabel("log N_x(r)")
            plt.title(f"Log-Log Plot (Token {token_idx})")
            plt.legend()

            plt.subplot(1, 2, 2)
            plt.plot(log_r, slopes, label="Slopes")
            for change in changes:
                plt.axvline(log_r[change], color='r', linestyle='--')
            plt.xlabel("log r")
            plt.ylabel("Slope")
            plt.title(f"Slopes with Changes (Token {token_idx})")
            plt.legend()
            plt.tight_layout()
            plt.show()

    if all_p_values:
        corrected = multipletests(all_p_values, alpha=alpha, method='holm')[1]

    # Print dimensions for all tokens
    print("\nDimensions for all tokens:")
    labels = [f"{tok} ({ctx.split()[1] if len(ctx.split()) > 1 else ctx})" for tok, ctx in zip(target_tokens, sentences)]
    for (token_idx, _), label, (base_dim, fiber_dim) in zip(results, labels, dimensions):
        if base_dim is not None and fiber_dim is not None:
            print(f"Token '{label}' (idx {token_idx}): Base={base_dim:.2f}, Fiber={fiber_dim:.2f}")
        else:
            print(f"Token '{label}' (idx {token_idx}): No significant change detected")

    return results

# Run the test
results = test_fiber_bundle_hypothesis(E_vecs, r_min=0.01, r_max=20.0, n_r=200, alpha=0.05, window_size=10)

# Print results with context
labels = [f"{tok} ({ctx.split()[1] if len(ctx.split()) > 1 else ctx})" for tok, ctx in zip(target_tokens, sentences)]
for (token_idx, result), label in zip(results, labels):
    print(f"Token '{label}' (idx {token_idx}): {result}")

rejections = sum(1 for _, r in results if r == "Reject")
print(f"Total tokens where fiber bundle hypothesis is rejected: {rejections}/{n_tokens}")