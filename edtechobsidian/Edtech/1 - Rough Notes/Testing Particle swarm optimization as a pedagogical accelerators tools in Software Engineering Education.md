# üìö Exploring Particle Swarm Optimization to Enhance Learning Efficiency in Software Engineering Courses - Research Summary

# **Methodology**

## **1. Research Design**

This study employs a **mixed-methods of experimental design** to evaluate the effectiveness of Particle Swarm Optimization (PSO) as a pedagogical accelerator in Software Engineering education. Two student groups are compared:

- **Control Group:** Completes an optimization-related software engineering task manually.
    
- **Experimental Group:** Completes the same task using a PSO-supported tool.
    

Both groups complete standardized assessments before and after the intervention, along with performance tasks and cognitive load measures. Qualitative reflections are collected to complement quantitative results.

---

## **2. Participants**

Participants are undergraduate Software Engineering or Computer Science students enrolled in a third-year course. Students are assigned to groups based on existing class sections to avoid scheduling conflicts, following a quasi-experimental structure.


# **3. Data Collection**

## **3.1 Collection Methods**

Data is collected using four channels:

1. **Pre-test and Post-test:** Measures conceptual understanding of optimization and software engineering problem-solving before and after the intervention.
    
2. **Performance Task:** Students complete an SE problem (e.g., test case generation, scheduling, or resource allocation) either manually or with PSO.
    
3. **Surveys:**
    
    - **NASA-TLX** for cognitive load
        
    - Short reflective questionnaire for student perceptions
        
4. **Logging Data:** IDE or tool timestamps are used to record task duration.
## **3.2 Instruments Used**

| Instrument                                                              | Purpose                                        |
| ----------------------------------------------------------------------- | ---------------------------------------------- |
| **Software Engineering Concept Inventory (20‚Äì30 items)**                | Measures learning gains                        |
| **Performance Rubric** (accuracy, efficiency, clarity, maintainability) | Scores solution quality                        |
| **NASA-TLX Cognitive Load Survey**                                      | Measures mental, physical, and temporal demand |
| **Reflection Questionnaire**                                            | Collects qualitative insights                  |
| **IDE Log/Timestamps**                                                  | Measures time-on-task                          |






## üìã Metadata
### Publication Details
- **Authors**: [Ross Ward]
- **Year**: [2025]
- **Journal/Source**: [TO BE PUBLISHED]
- **DOI/URL**: [Digital identifier]
- **Citation**: [Full citation]

### Research Type
- **Methodology**: [e.g., Quantitative, i.e. Task completion time, Accuracy of code or design, Number of defects found, Quality of software artifacts, Improvement from pre-test to post-test, Cognitive load scores, Qualitative, - Student understanding, Reasoning process, Perceived usefulness of PSO, Conceptual clarity, Engagement Mixed Method,]
- **Study Design**: [e.g., Experimental, Case Study, Survey]
- **Sample Size**: [Number of participants]
- **Duration**: [Study duration]

## üéØ Research Focus
### Research Questions
1. [Primary research question]
2. [Secondary research question(s)]
3. [Additional questions]

### Hypotheses
1. [Primary hypothesis]
2. [Secondary hypothesis/hypotheses]

## üìä Methodology
Anchor the study around specific measurable pedagogical tools:
Concept : Concept Inventory - 1Q : Does the user have learning gain more learning gain using agents using specific swarm design patterns vs traditional.
Pre-test/Post-test -> Performance Rubric -> NASA-TLX Cognitive Load Survey
The aim is to gain valuable insight into student learning gain, performance quality, student cognitive load and overall student perception of learning through Agentic software design.
### Data Collection
This will involve a mixed approach with experimental design solutions in which we give students tools involving two types of tools.
Control group : Can be assisted with or without using traditional LLM or agent
Experimental group : Uses LLM Tool specifically trained and optimized by Particle Swarm Optimization (PSO)

Both groups Use Pre-test, Treatment and post-test with performance task scored using a standardized rubric, students cognitive load will be tested using a NASA-TLX instrument with short open-ended reflections 

### **1. Collection Methods**

- **Pre-test/Post-test administration** (measures learning gain)
- **Performance task execution** (code/design task solved manually or with PSO)
- **Rubric-based scoring** by two independent evaluators
- **NASA-TLX cognitive load survey** (Likert)
- **Short reflection questionnaire** (qualitative insights)
- **Optional:** System logs for time-on-task
    

---

### **2. Instruments Used**

|Instrument|Purpose|
|---|---|
|**SE Concept Inventory (custom 20‚Äì30 items)**|Measures conceptual understanding before/after|
|**Performance Rubric (accuracy, efficiency, code quality)**|Measures solution quality|
|**NASA-TLX**|Measures cognitive load during the task|
|**Reflection Form (3‚Äì4 open questions)**|Captures perceptions and learning experience|
|**IDE logging / timestamps**|Measures task duration|

---

### **3. Timeline**

|Week|Activity|
|---|---|
|Week 1|Pre-test + orientation|
|Week 2|Manual task (control) and PSO-supported task (experimental)|
|Week 3|Post-test + NASA-TLX administration|
|Week 4|Reflection collection + evaluator scoring|
|Week 5|Data cleaning + analysis|

Total realistic duration: **4‚Äì5 weeks**

---

### **4. Validation Approach**

To ensure quality and credibility:

- **Pilot testing** of concept inventory items
    
- **Rubric validation** through expert review (2‚Äì3 instructors)
    
- **Inter-rater reliability** (Cohen‚Äôs Œ∫) for rubric scoring
    
- **Cronbach‚Äôs alpha** for NASA-TLX internal consistency
    
- **Member checking** for qualitative reflections (optional)

**Analysis Methods**
### **1. Statistical Techniques (Quantitative)**

| Goal                                  | Technique                                                                  |
| ------------------------------------- | -------------------------------------------------------------------------- |
| Compare learning gains                | **Paired t-test** (within groups), **Independent t-test** (between groups) |
| Compare performance scores            | TBD                                                                        |
| Measure effect size                   | TBD                                                                        |
| Cognitive load comparison             | **Independent t-test**                                                     |
| Correlation (PSO use vs. improvement) | **TBD correlation**                                                        |

All common, accepted methods in SE education research.

---

### **2. Analytical Framework (Qualitative)**

Use **Thematic Analysis (Braun & Clarke)**, which includes:

1. Familiarization
    
2. Coding
    
3. Theme development
    
4. Review
    
5. Define & name themes
    
6. Report
    

Themes often include:

- Perceived ease of problem-solving
    
- Engagement
    
- Conceptual clarity
    
- Challenges in using PSO
    

---

### **3. Tools Used**

- TBD ‚Üí quantitative analysis
    
- TBD ‚Üí qualitative coding
    
- TBD ‚Üí data organization
    
- **Git/IDE logs** ‚Üí time measurement
    

---

### **4. Validation Process**

- **Triangulation:** Compare quantitative + qualitative results
    
- **Reliability checks:**
    
    - Inter-rater reliability (rubric)
        
    - Cronbach‚Äôs alpha (surveys)
        
- **Assumption testing:**
    
    - Shapiro‚ÄìWilk for normality
        
    - Levene‚Äôs test for homogeneity of variance
        
- **Peer debriefing:** Review interpretations with another researcher
    
- **Audit trail:** Document coding decisions
## üí° Key Findings

### Primary Outcomes
1. [Major finding 1]
2. [Major finding 2]
3. [Major finding 3]

### Secondary Outcomes
1. [Additional finding 1]
2. [Additional finding 2]
3. [Additional finding 3]

## üîç Critical Analysis

### Strengths
- [Strength 1]
- [Strength 2]
- [Strength 3]

### Limitations
- [Limitation 1]
- [Limitation 2]
- [Limitation 3]

### Implications
- [Implication 1]
- [Implication 2]
- [Implication 3]

## üîÑ Applications

### Practical Applications
1. [Application area 1]
2. [Application area 2]
3. [Application area 3]

### Future Research
- [Research direction 1]
- [Research direction 2]
- [Research direction 3]

## üîó Related Research
### Similar Studies
- [[Related Study 1]]
- [[Related Study 2]]
- [[Related Study 3]]

### Connected Topics
- [[Topic 1]]
- [[Topic 2]]
- [[Topic 3]]

## üìö Additional Resources
### Referenced Works
- [Key reference 1]
- [Key reference 2]
- [Key reference 3]

### Supplementary Materials
- Data Sets
- Research Instruments
- Additional Documentation

## üè∑Ô∏è Tags
#ResearchSummary #Methodology #Domain #Findings #Applications

## üìù Notes
- Research Quality Rating: [Rating]
- Replication Potential: [Assessment]
- Implementation Considerations: [Notes]
- Review Comments: [Additional thoughts]

---
Created: {{date}}
Last Updated: {{date}}
Review Frequency: Annually 