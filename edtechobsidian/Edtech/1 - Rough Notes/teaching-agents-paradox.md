## **"Teaching Students to Fish with AI: Does Agent Development Foster Independence or Dependency in Software Education?"**

**Alternative titles:**
- "The Agent Builder's Paradox: Self-Reliance Through AI Literacy"
- "From Consumers to Creators: Teaching Agentic Design Patterns for Independent Learning"

**What to study:**

- Independent problem-solving capability
- Content generation patterns (self vs AI)
- Agent building proficiency
- Learning transfer to novel problems

**Why:**

- Students who build agents understand AI limitations better
- Meta-skill of "teaching the teacher" may enhance learning
- Risk of automation dependency vs empowerment

### **What this study investigates**

- How teaching **agentic design patterns** affects:
    - Student self-authorship vs AI-generated content
    - Independent learning capabilities
    - Problem decomposition skills
    - Critical evaluation of AI outputs

### **Why this matters**

- "Give a man a fish" vs "teach to fish" applied to AI
- Students who understand agent architecture may use AI more strategically
- Building agents requires deeper understanding than using them
- Meta-cognitive benefits of teaching AI to solve problems

### **The Central Paradox**

**Concern**: Teaching students to build AI agents will make them dependent on automation for all content creation

**Counter-hypothesis**: Students who build agents develop:
- Deeper understanding of problem decomposition
- Critical evaluation skills for AI outputs
- Strategic tool usage (when to automate vs manual)
- Meta-cognitive awareness of learning process

### **Relevant Theories**

- **Constructionism** (Papert) - learning by building
- **Metacognition** (Flavell) - thinking about thinking
- **Transfer of Learning** (Perkins & Salomon)
- **Cognitive Apprenticeship** (Collins, Brown, Newman)
- **Self-Regulated Learning** (Zimmerman)

### **Core Research Question**

Does teaching students to build AI agents using agentic design patterns lead to:
1. **Dependency**: Over-reliance on AI for all content generation?
2. **Independence**: Enhanced problem-solving and strategic AI usage?

|Construct|Metric / Instrument|
|---|---|
|Content Authorship|Ratio of self-written vs AI-generated (tracked)|
|Problem Decomposition|Task breakdown quality rubric|
|Agent Building Skill|Pattern implementation assessment|
|Independent Learning|Novel problem-solving without AI|
|Critical Evaluation|AI output verification accuracy|
|Strategic Usage|Decision logs (when to use AI)|
|Metacognition|Self-reflection journals, MAI survey|

### **Methodology**

- **Quasi-experimental** (semester-long course)
- **Control**: Traditional AI tool usage (ChatGPT, Copilot)
- **Experimental**: Agent building curriculum (ADP focus)
- Mixed methods (quantitative + qualitative)
- Longitudinal tracking (3 assessment points)

---

## **Extended Literature Foundation**

### **The "Teaching to Fish" Metaphor in AI Education**

Traditional proverb: "Give a man a fish, feed him for a day. Teach him to fish, feed him for life."

**AI adaptation**: 
- **Give AI tools** = Students use ChatGPT/Copilot as black boxes
- **Teach agent building** = Students understand how to construct, constrain, and evaluate AI systems

Recent research shows concerning patterns:
- Students copy-paste AI code without understanding (IEEE Spectrum, 2024)
- Over-reliance on AI assistants reduces debugging skills (MDPI, 2024)
- "AI coding was changing... the world of AI-assisted coding intensified" (ZDNET, 2025)

**But**: "Unlike AI assistants like ChatGPT and Copilot, AI-powered learning is designed to support new coders" (Medium, 2024)

### **Constructionism: Learning by Building**

Seymour Papert's constructionism argues **building artifacts deepens understanding**:

- Students who program robots understand physics better
- Creating simulations enhances conceptual grasp
- **Building AI agents** may similarly deepen AI literacy

"GenAI has the ability to simplify syntax, generate illustrative examples, and streamline development processes" (ResearchGate, 2024) - but does building these systems teach more than using them?

### **Metacognition and AI Literacy**

**Metacognition** = thinking about one's own thinking

Teaching agent building requires students to:
1. **Plan**: Decompose problems into agent-solvable tasks
2. **Monitor**: Evaluate agent performance and outputs
3. **Evaluate**: Assess when agents succeed/fail and why

"By 2025, AI will be deeply integrated into the software lifecycle automating up to 30% of coding tasks" (Technostacks, 2025) - students need metacognitive skills to manage this integration.

### **Agentic Design Patterns as Pedagogical Framework**

From Antonio Gulli's framework, patterns that teach meta-skills:

1. **Prompt Chaining**: Teaches task decomposition
2. **Reflection**: Teaches self-evaluation loops
3. **Planning**: Teaches goal-oriented thinking
4. **Tool Use**: Teaches strategic resource selection
5. **Human-in-the-Loop**: Teaches critical oversight
6. **Evaluation & Monitoring**: Teaches quality assessment

**Key insight**: Building these patterns requires understanding the problem at a deeper level than just using AI.

### **Transfer of Learning**

**Near transfer**: Skills apply to similar contexts
**Far transfer**: Skills apply to novel domains

**Hypothesis**: Agent building teaches far transfer skills:
- Problem decomposition transfers to non-AI contexts
- Critical evaluation transfers to all information sources
- Strategic tool usage transfers to professional practice

### **Current Research Gaps**

- Most studies focus on AI **usage**, not AI **creation**
- Limited research on agent building as pedagogy
- No comparative studies: consumers vs creators
- Unclear whether building agents prevents dependency

---

## **Theoretical Framework**

### **Constructionism (Papert, 1980)**

**Core principle**: "You learn best when you build something meaningful"

**Application**: 
- Students don't just use ChatGPT
- They build their own specialized agents
- Process of building = deeper learning

**Example**: Student building a "code review agent" must understand:
- What makes good code
- How to evaluate quality
- When automated review fails

### **Metacognitive Development**

**Flavell's Model**:
- **Metacognitive knowledge**: Understanding AI capabilities/limits
- **Metacognitive regulation**: Monitoring AI usage effectiveness
- **Metacognitive experiences**: Reflecting on learning with AI

**Agent building enhances all three**:
- Knowledge: Learn what AI can/cannot do by building it
- Regulation: Monitor agent performance, adjust prompts
- Experience: Reflect on when agents help vs hinder

### **Cognitive Load Theory (Sweller)**

**Concern**: Does agent building add excessive cognitive load?

**Counter**: Agent building may **reduce** long-term load by:
- Creating reusable mental models
- Automating routine tasks strategically
- Freeing cognitive resources for complex problems

**Germane load**: Agent building is "productive struggle" that builds schemas

### **Self-Regulated Learning (Zimmerman)**

**Three phases**:
1. **Forethought**: Planning agent architecture
2. **Performance**: Building and testing agents
3. **Self-reflection**: Evaluating agent effectiveness

**Agent building naturally scaffolds SRL**:
- Students must plan before building
- Monitor during development
- Reflect on outcomes

---

## **Study Design**

### **Participants**

- **N = 80** undergraduate CS students (2nd/3rd year)
- **Control Group (n=40)**: Traditional AI tool usage
  - Access to ChatGPT, GitHub Copilot, Claude
  - Standard "how to prompt" training
- **Experimental Group (n=40)**: Agent building curriculum
  - Learn agentic design patterns
  - Build 3 custom agents during semester
- **Duration**: 12-week semester course

### **Course Structure**

#### **Control Group: "AI-Assisted Programming"**

**Weeks 1-3**: Prompt engineering basics
- Effective prompting techniques
- Code generation with Copilot
- Debugging with ChatGPT

**Weeks 4-9**: Project work with AI tools
- Use AI for assignments
- AI-assisted debugging
- AI-generated documentation

**Weeks 10-12**: Final project
- Build application using AI tools
- Reflection on AI usage

#### **Experimental Group: "Building AI Agents"**

**Weeks 1-3**: Agentic design patterns
- Prompt chaining
- Reflection loops
- Tool use patterns

**Weeks 4-6**: Agent 1 - Personal Learning Assistant
- Build agent that helps with coursework
- Implement reflection pattern
- Evaluate agent effectiveness

**Weeks 7-9**: Agent 2 - Code Review Agent
- Build agent that reviews code quality
- Implement evaluation pattern
- Compare to manual review

**Weeks 10-12**: Agent 3 - Custom Domain Agent
- Student chooses domain
- Applies multiple patterns
- Presents agent architecture

### **Assessment Points**

**Time 1 (Week 1)**: Baseline
- Problem-solving assessment (no AI)
- AI literacy survey
- Metacognitive awareness inventory

**Time 2 (Week 6)**: Midpoint
- Content authorship analysis
- Independent problem-solving task
- Agent building assessment (experimental only)

**Time 3 (Week 12)**: Final
- Novel problem-solving (no AI allowed)
- Content authorship analysis
- Transfer task (apply skills to new domain)
- Qualitative interviews

---

## **Data Collection Instruments**

### **Quantitative Measures**

| Measure | Instrument | Collection Method |
|---------|-----------|-------------------|
| Content Authorship | Code analysis: self vs AI-generated | Git commits + AI tool logs |
| Problem Decomposition | Rubric: task breakdown quality | Assignment submissions |
| Independent Problem-Solving | Timed challenges (no AI) | Proctored assessments |
| Agent Building Skill | Pattern implementation rubric | Agent code review |
| AI Literacy | Custom survey (Likert scale) | Pre/mid/post |
| Metacognition | MAI (Metacognitive Awareness Inventory) | Pre/post |
| Strategic Usage | Decision logs + rationale | Weekly reflections |
| Transfer of Learning | Novel domain problem | Final assessment |

### **Qualitative Measures**

1. **Weekly Reflection Journals**
   - When did you use AI vs manual work?
   - Why did you make that choice?
   - What did you learn this week?

2. **Semi-Structured Interviews** (n=20, stratified sample)
   - How has your relationship with AI changed?
   - Do you feel more/less dependent on AI?
   - What skills have you developed?

3. **Think-Aloud Protocols** (n=10 per group)
   - Solve problem while verbalizing thought process
   - Analyze decision-making patterns

### **Content Authorship Analysis**

**Automated tracking**:
- Git commit analysis (code churn, complexity)
- AI tool API logs (prompts, generations, acceptances)
- Time-on-task metrics

**Classification**:
- **Self-authored**: Written without AI assistance
- **AI-assisted**: AI suggestions, human edited
- **AI-generated**: Minimal human modification
- **Hybrid**: Significant collaboration

**Key metric**: Ratio of self-authored to AI-generated over time

---

## **Expected Findings**

### **Hypothesis 1: The Builder's Advantage**

**Experimental group will show**:
- Higher quality AI-generated content (better prompts)
- More strategic AI usage (selective, not universal)
- Better critical evaluation of AI outputs
- Maintained independent problem-solving skills

**Mechanism**: Building agents requires understanding problems deeply, leading to better AI usage

### **Hypothesis 2: Content Authorship Patterns**

**Control group**:
- Increasing AI-generated content over time
- Decreasing self-authored code
- Lower quality AI outputs (poor prompts)

**Experimental group**:
- Stable or increasing self-authored content
- Strategic AI usage for appropriate tasks
- Higher quality AI outputs (better prompts)

**Visualization**:
```
Content Authorship Over Time

Control:     [Self ████████░░░░░░░░] → [Self ███░░░░░░░░░░░░░]
Experimental: [Self ████████░░░░░░░░] → [Self ███████░░░░░░░░]

Legend: █ = Self-authored, ░ = AI-generated
```

### **Hypothesis 3: Metacognitive Development**

**Experimental group will show**:
- Higher metacognitive awareness scores
- Better self-regulation strategies
- More accurate self-assessment
- Stronger transfer to novel problems

**Evidence**: MAI scores, reflection journal quality, interview themes

### **Hypothesis 4: The "Teaching Effect"**

Students who build agents will demonstrate:
- Deeper understanding of AI limitations
- More realistic expectations of AI capabilities
- Better debugging of AI failures
- Enhanced problem decomposition skills

**Mechanism**: Teaching AI to solve problems requires understanding the problem structure

---

## **Pedagogical Implications**

### **The "Fish" Metaphor Resolved**

**Traditional AI usage** = Giving fish
- Students get immediate solutions
- Limited understanding of process
- Dependency on tool availability

**Agent building** = Teaching to fish
- Students learn problem decomposition
- Understand AI capabilities/limits
- Strategic tool usage
- Transferable meta-skills

**But also**: Teaching to **build fishing rods**
- Students create custom tools
- Adapt tools to specific contexts
- Maintain tools over time

### **Curriculum Design Principles**

1. **Start with building, not using**
   - Teach agent architecture before tool usage
   - Understand components before black-box usage

2. **Scaffold pattern learning**
   - Begin with simple patterns (prompt chaining)
   - Progress to complex (multi-agent systems)

3. **Require reflection**
   - Weekly journals on AI usage decisions
   - Explicit metacognitive prompts

4. **Maintain manual skills**
   - Regular no-AI assessments
   - Balance automation with fundamentals

5. **Emphasize strategic usage**
   - When to use AI vs manual work
   - Cost-benefit analysis of automation

### **For Educators**

**Don't**: Ban AI or allow unrestricted use

**Do**: Teach students to build AI systems
- Deeper understanding through construction
- Critical evaluation through debugging
- Strategic usage through experience

**Assessment redesign**:
- Test independent problem-solving (no AI)
- Evaluate agent building quality
- Assess strategic decision-making
- Measure transfer to novel domains

### **For Students**

**Skills developed through agent building**:

1. **Problem Decomposition**
   - Break complex tasks into agent-solvable steps
   - Identify automation boundaries

2. **Critical Evaluation**
   - Assess AI output quality
   - Debug agent failures
   - Verify correctness

3. **Strategic Thinking**
   - When to automate vs manual work
   - Cost-benefit of AI usage
   - Long-term vs short-term gains

4. **Meta-Learning**
   - Understanding own learning process
   - Adapting strategies based on feedback
   - Transferring skills to new contexts

---

## **Potential Risks & Mitigation**

### **Risk 1: Increased Cognitive Load**

**Concern**: Agent building adds complexity to already challenging coursework

**Mitigation**:
- Scaffold pattern learning gradually
- Provide templates and examples
- Focus on 3-4 core patterns, not all
- Balance with reduced other coursework

### **Risk 2: Technical Barriers**

**Concern**: Students struggle with agent implementation details

**Mitigation**:
- Use high-level frameworks (LangChain, AutoGen)
- Provide starter code and templates
- Focus on design patterns, not implementation
- Pair programming for support

### **Risk 3: Still Leads to Dependency**

**Concern**: Students build agents then over-rely on them

**Mitigation**:
- Regular no-AI assessments
- Require manual problem-solving
- Teach when NOT to use agents
- Emphasize agent limitations

### **Risk 4: Inequitable Access**

**Concern**: Some students have more resources for agent building

**Mitigation**:
- Provide institutional API access
- Use open-source models (Llama, Mistral)
- Ensure equal computational resources
- Support for diverse learning needs

---

## **Alternative Outcomes & Interpretations**

### **Scenario A: No Difference**

**Finding**: Both groups show similar dependency/independence

**Interpretation**: 
- Agent building doesn't provide advantage
- AI usage patterns driven by other factors
- Need different pedagogical approach

**Implications**: Reconsider agent building as pedagogy

### **Scenario B: Control Group More Independent**

**Finding**: Control group maintains more self-authorship

**Interpretation**:
- Agent building increases automation comfort
- Experimental group over-automates
- Building agents ≠ strategic usage

**Implications**: Need stronger scaffolds for strategic usage

### **Scenario C: Experimental Group More Dependent**

**Finding**: Experimental group relies more on AI

**Interpretation**:
- Building agents normalizes automation
- Students automate everything they can
- "When you have a hammer..." effect

**Implications**: Teach explicit boundaries and limitations

### **Scenario D: Confirmed Hypothesis**

**Finding**: Experimental group more independent and strategic

**Interpretation**:
- Agent building teaches meta-skills
- Understanding architecture enables better usage
- Constructionism principle validated

**Implications**: Scale agent building curriculum

---

## **Future Research Directions**

### **Longitudinal Studies**

- Track students 1-2 years post-course
- Do agent building skills persist?
- Professional practice outcomes
- Career trajectory analysis

### **Pattern-Specific Effects**

- Which agentic patterns most beneficial?
- Optimal pattern learning sequence
- Pattern combinations and interactions
- Domain-specific pattern effectiveness

### **Cross-Disciplinary Applications**

- Does agent building help in:
  - Data science education?
  - Web development?
  - Mobile app development?
  - Non-CS fields (writing, design)?

### **Cognitive Mechanisms**

- What cognitive processes mediate effects?
- Role of working memory capacity
- Prior knowledge interactions
- Individual differences in learning

### **Optimal Scaffolding**

- How much guidance needed?
- When to remove scaffolds?
- Adaptive scaffolding based on performance
- Peer collaboration effects

### **Industry Validation**

- Do employers value agent building skills?
- Professional practice transfer
- Workplace AI usage patterns
- Career advancement outcomes

---

## **Practical Implementation Guide**

### **Week-by-Week Curriculum**

**Week 1: Introduction to Agentic Thinking**
- What are agents vs tools?
- Design patterns overview
- First simple agent (prompt chain)

**Week 2: Prompt Chaining Pattern**
- Multi-step problem decomposition
- Sequential agent design
- Build: Study guide generator

**Week 3: Reflection Pattern**
- Self-evaluation loops
- Quality assessment
- Build: Code reviewer (basic)

**Week 4-6: Project 1 - Learning Assistant**
- Combine patterns learned
- Personal domain choice
- Presentation and peer review

**Week 7: Tool Use Pattern**
- When to use external tools
- API integration
- Build: Research assistant

**Week 8: Planning Pattern**
- Goal-oriented agents
- Task decomposition
- Build: Project planner

**Week 9: Human-in-the-Loop**
- Critical decision points
- Verification strategies
- Build: Code reviewer (advanced)

**Week 10-12: Final Project - Custom Agent**
- Student-chosen domain
- Multiple pattern integration
- Documentation and presentation

### **Assessment Rubric for Agent Building**

| Criterion | Novice (1-2) | Developing (3-4) | Proficient (5-6) | Expert (7-8) |
|-----------|--------------|------------------|------------------|--------------|
| Pattern Implementation | Single pattern, basic | Multiple patterns, some integration | Well-integrated patterns | Novel pattern combinations |
| Problem Decomposition | Unclear task breakdown | Basic decomposition | Clear, logical steps | Optimal decomposition |
| Critical Evaluation | No output verification | Basic testing | Systematic evaluation | Comprehensive validation |
| Documentation | Minimal explanation | Basic documentation | Clear documentation | Exemplary documentation |
| Strategic Usage | Automates everything | Some selectivity | Strategic choices | Optimal automation boundaries |

---

## **Ethical Considerations**

### **Academic Integrity**

**Challenge**: How to assess learning when students build AI agents?

**Approach**:
- Require agent architecture documentation
- Explain design decisions
- No-AI assessments for fundamentals
- Code explanation requirements

### **Equity & Access**

**Challenge**: Not all students have equal AI access

**Approach**:
- Institutional API keys
- Open-source model options
- Computational resource provision
- Alternative assignments if needed

### **Skill Displacement**

**Challenge**: Are we teaching students to automate themselves out of jobs?

**Approach**:
- Emphasize human judgment skills
- Teach AI limitations explicitly
- Focus on human-AI collaboration
- Prepare for AI-augmented careers

### **Responsible AI Development**

**Challenge**: Students may build harmful agents

**Approach**:
- Ethics module in curriculum
- Guardrails and safety patterns
- Review agent purposes
- Discuss societal implications

---

## **Conclusion**

### **The Paradox Resolved**

Teaching students to build AI agents may seem to risk dependency, but evidence suggests the opposite:

**Building ≠ Using**
- Builders understand limitations
- Builders evaluate critically
- Builders use strategically

**The "Teaching to Fish" metaphor holds**:
- Using AI = receiving fish
- Building agents = learning to fish
- Understanding patterns = teaching others to fish

**Meta-skill development**:
- Problem decomposition
- Critical evaluation
- Strategic tool usage
- Transfer to novel domains

### **Key Takeaway**

**Don't ask**: "Will students become dependent on AI?"

**Ask instead**: "How can we teach students to build AI systems that enhance their independence?"

**Answer**: Through agentic design patterns that scaffold metacognition, strategic thinking, and transferable problem-solving skills.

---

## **References**

### **AI in Education (2024-2025)**

- IEEE Spectrum. (2024). AI Copilots Are Changing How Coding Is Taught. https://spectrum.ieee.org/ai-coding
- MDPI. (2024). The Good and Bad of AI Tools in Novice Programming Education. Education Sciences, 14(10), 1089.
- Medium. (2024). How to learn to code with AI in 2024. https://medium.com/@fahimulhaq/how-to-learn-to-code-with-ai-in-2024
- ResearchGate. (2024). Cracking the code: Co-coding with AI in creative programming education.
- Technostacks. (2025). How AI Is Revolutionizing Coding & Developer Productivity in 2025.
- ZDNET. (2025). The best free AI for coding in 2025. https://www.zdnet.com/article/the-best-free-ai-for-coding-in-2025

### **Theoretical Foundations**

- Collins, A., Brown, J. S., & Newman, S. E. (1989). Cognitive apprenticeship: Teaching the crafts of reading, writing, and mathematics. In L. B. Resnick (Ed.), Knowing, learning, and instruction: Essays in honor of Robert Glaser (pp. 453-494). Lawrence Erlbaum Associates.
- Flavell, J. H. (1979). Metacognition and cognitive monitoring: A new area of cognitive-developmental inquiry. American Psychologist, 34(10), 906-911.
- Gulli, A. (2024). Agentic Design Patterns: A Hands-On Guide to Building Intelligent Systems.
- Papert, S. (1980). Mindstorms: Children, computers, and powerful ideas. Basic Books.
- Perkins, D. N., & Salomon, G. (1992). Transfer of learning. International Encyclopedia of Education, 2, 6452-6457.
- Sweller, J. (1988). Cognitive load during problem solving: Effects on learning. Cognitive Science, 12(2), 257-285.
- Zimmerman, B. J. (2002). Becoming a self-regulated learner: An overview. Theory Into Practice, 41(2), 64-70.

### **Constructionism & Learning**

- Kafai, Y., & Resnick, M. (Eds.). (1996). Constructionism in practice: Designing, thinking, and learning in a digital world. Lawrence Erlbaum Associates.
- Papert, S., & Harel, I. (1991). Situating constructionism. In S. Papert & I. Harel (Eds.), Constructionism (pp. 1-11). Ablex Publishing.

### **Metacognition Research**

- Schraw, G., & Dennison, R. S. (1994). Assessing metacognitive awareness. Contemporary Educational Psychology, 19(4), 460-475.
- Veenman, M. V., Van Hout-Wolters, B. H., & Afflerbach, P. (2006). Metacognition and learning: Conceptual and methodological considerations. Metacognition and Learning, 1(1), 3-14.

### **Transfer of Learning**

- Barnett, S. M., & Ceci, S. J. (2002). When and where do we apply what we learn? A taxonomy for far transfer. Psychological Bulletin, 128(4), 612-637.
- Bransford, J. D., & Schwartz, D. L. (1999). Rethinking transfer: A simple proposal with multiple implications. Review of Research in Education, 24, 61-100.
