# -*- coding: utf-8 -*-
"""Exploring LORE Fine Tuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1REoLDdX6xou9AAT5FLhjiFWx--Go25o3
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# --- Configuration ---
EMBEDDING_DIM = 4096
RANK_B = 10
LEARNING_RATE_BASIS = 0.0005
LEARNING_RATE_FEWSHOT = 0.01
NUM_SEEN_USERS = 100
NUM_UNSEEN_USERS = 10
EPOCHS_BASIS = 5
EPOCHS_FEWSHOT = 10

# --- Placeholder/Dummy Components ---

class PreTrainedEmbeddingModel(nn.Module):
    """Placeholder for a frozen pre-trained model that generates embeddings."""
    def __init__(self, embedding_dim):
        super().__init__()
        self.embedding_dim = embedding_dim
        print(f"Initialized Placeholder Embedding Model (Dim: {self.embedding_dim})")

    def forward(self, prompt_response_pair):
        batch_size = len(prompt_response_pair)
        return torch.randn(batch_size, self.embedding_dim)

class PreferenceDataset(Dataset):
    """Placeholder for loading preference data (x, yc, yr) for a user."""
    def __init__(self, user_data):
        self.data = user_data
        print(f"Initialized Placeholder Dataset with {len(self.data)} samples.")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        prompt, yc, yr = self.data[idx]
        return (prompt, yc), (prompt, yr)

# --- LoRe Model Definition ---

class LoReRewardModel(nn.Module):
    """Implements the LoRe framework core logic."""
    def __init__(self, embedding_model, embedding_dim, rank_b, num_seen_users):
        super().__init__()
        self.embedding_model = embedding_model
        for param in self.embedding_model.parameters():
            param.requires_grad = False

        self.reward_basis_projection = nn.Linear(embedding_dim, rank_b, bias=False)
        self.user_weights = nn.Parameter(torch.randn(num_seen_users, rank_b))
        print(f"Initialized LoRe Model (Rank B={rank_b}, Seen Users={num_seen_users})")

    def get_reward_basis_output(self, prompt_response_pair):
        """Calculates the B-dimensional reward basis output R_phi(x, y)."""
        with torch.no_grad():
             embeddings = self.embedding_model(prompt_response_pair)
        basis_output = self.reward_basis_projection(embeddings)
        return basis_output

    def get_personalized_reward(self, prompt_response_pair, user_id_or_weights_param):
        """Calculates the personalized scalar reward p_i(x, y) = w_i^T * R_phi(x, y)."""
        basis_output = self.get_reward_basis_output(prompt_response_pair)

        if isinstance(user_id_or_weights_param, int): # Seen user ID
            user_w = self.user_weights[user_id_or_weights_param]
        elif isinstance(user_id_or_weights_param, nn.Parameter): # New user weights parameter (requires grad)
             user_w = user_id_or_weights_param
        elif isinstance(user_id_or_weights_param, torch.Tensor): # Inference with detached weights
             user_w = user_id_or_weights_param
        else:
            raise ValueError("Provide user_id (int), user_weights (nn.Parameter or Tensor)")

        if user_w.dim() == 1:
            user_w = user_w.unsqueeze(0)

        personalized_reward = torch.sum(basis_output * user_w, dim=1)
        return personalized_reward

    # ***** Correction: Updated signature description *****
    def forward(self, chosen_pair, rejected_pair, user_id_or_weights_param):
        """Calculates the reward difference for the BT loss.
           user_id_or_weights_param: Can be int (user_id), nn.Parameter (new weights), or Tensor (learned weights)"""
        reward_chosen = self.get_personalized_reward(chosen_pair, user_id_or_weights_param)
        reward_rejected = self.get_personalized_reward(rejected_pair, user_id_or_weights_param)
        return reward_chosen - reward_rejected

# --- Loss Function ---
def logistic_loss(reward_diff):
    """Calculates log(1 + exp(-reward_diff))."""
    return torch.log(1 + torch.exp(-reward_diff))

# --- Training Functions ---

def train_basis_and_weights(model, seen_user_dataloaders, optimizer, epochs):
    """Jointly trains the reward basis (projection A) and seen user weights W."""
    model.train()
    num_users = len(seen_user_dataloaders)
    print(f"\n--- Starting Joint Training (Basis A and {num_users} User Weights W) ---")

    for epoch in range(epochs):
        total_loss = 0
        num_samples = 0
        for user_id, dataloader in enumerate(seen_user_dataloaders):
            user_epoch_loss = 0
            user_samples = 0
            for batch_idx, (chosen_pair_batch, rejected_pair_batch) in enumerate(dataloader):
                optimizer.zero_grad()
                reward_differences = model(chosen_pair_batch, rejected_pair_batch, user_id) # Pass user_id positionally
                loss = logistic_loss(reward_differences).mean()
                loss.backward()
                optimizer.step()
                user_epoch_loss += loss.item() * len(chosen_pair_batch)
                user_samples += len(chosen_pair_batch)

            total_loss += user_epoch_loss
            num_samples += user_samples

        if num_samples > 0:
            avg_epoch_loss = total_loss / num_samples
            print(f"Epoch {epoch+1}/{epochs} - Overall Avg Loss: {avg_epoch_loss:.4f}")
        else:
            print(f"Epoch {epoch+1}/{epochs} - No samples processed.")
    print("--- Joint Training Finished ---")


def few_shot_learn_new_user_weights(model, new_user_weights_param, few_shot_dataloader, optimizer_fewshot, epochs):
    """Learns weights w_new for an unseen user using few-shot data, keeping basis A frozen."""
    model.train()
    for param in model.reward_basis_projection.parameters():
        param.requires_grad = False
    if hasattr(model, 'user_weights'):
         model.user_weights.requires_grad_(False)
    new_user_weights_param.requires_grad_(True)

    print("\n--- Starting Few-Shot Adaptation for New User ---")
    for epoch in range(epochs):
        epoch_loss = 0
        num_samples = 0
        for batch_idx, (chosen_pair_batch, rejected_pair_batch) in enumerate(few_shot_dataloader):
             optimizer_fewshot.zero_grad()
             # Pass nn.Parameter positionally
             reward_differences = model(chosen_pair_batch, rejected_pair_batch, new_user_weights_param)
             loss = logistic_loss(reward_differences).mean()
             loss.backward()
             optimizer_fewshot.step()
             epoch_loss += loss.item() * len(chosen_pair_batch)
             num_samples += len(chosen_pair_batch)

        if num_samples > 0:
            avg_epoch_loss = epoch_loss / num_samples
            print(f"  Epoch {epoch+1}/{epochs} - Few-Shot Avg Loss: {avg_epoch_loss:.4f}")
        else:
             print(f"Epoch {epoch+1}/{epochs} - No few-shot samples processed.")

    print("--- Few-Shot Adaptation Finished ---")

    for param in model.reward_basis_projection.parameters():
        param.requires_grad = True
    if hasattr(model, 'user_weights'):
         model.user_weights.requires_grad_(True)

    learned_weights = new_user_weights_param.data.detach().clone()
    return learned_weights

# --- Example Usage ---

# 1. Initialize Models and Optimizer
embedding_model = PreTrainedEmbeddingModel(EMBEDDING_DIM)
lore_model = LoReRewardModel(embedding_model, EMBEDDING_DIM, RANK_B, NUM_SEEN_USERS)

optimizer_basis = optim.Adam(
    [
        {'params': lore_model.reward_basis_projection.parameters()},
        {'params': lore_model.user_weights}
    ],
    lr=LEARNING_RATE_BASIS
)

# 2. Load Dummy Data for Seen Users
seen_user_datasets = [
    PreferenceDataset([("prompt1", "good_respA", "bad_respB"), ("prompt2", "good_respC", "bad_respD")])
    for _ in range(NUM_SEEN_USERS)
]
seen_user_dataloaders = [DataLoader(ds, batch_size=2) for ds in seen_user_datasets]

# 3. Train Basis (A) and Seen User Weights (W) Jointly
train_basis_and_weights(lore_model, seen_user_dataloaders, optimizer_basis, EPOCHS_BASIS)

# 4. Few-Shot Adaptation for a New User
few_shot_data = PreferenceDataset([
    ("new_prompt1", "unseen_good1", "unseen_bad1"),
    ("new_prompt2", "unseen_good2", "unseen_bad2"),
    ("new_prompt3", "unseen_good3", "unseen_bad3")
])
few_shot_dataloader = DataLoader(few_shot_data, batch_size=2)

new_user_weights_param = nn.Parameter(torch.randn(1, RANK_B))
optimizer_fewshot = optim.Adam([new_user_weights_param], lr=LEARNING_RATE_FEWSHOT)

# Learn weights w_new
new_user_learned_weights = few_shot_learn_new_user_weights(
    model=lore_model,
    new_user_weights_param=new_user_weights_param,
    few_shot_dataloader=few_shot_dataloader,
    optimizer_fewshot=optimizer_fewshot,
    epochs=EPOCHS_FEWSHOT
)
print(f"\nLearned weights for new user: {new_user_learned_weights.numpy()}")

# 5. Inference/Prediction
lore_model.eval()
with torch.no_grad():
    test_pair_chosen = [("test_prompt", "candidate_response1")]
    test_pair_rejected = [("test_prompt", "candidate_response2")]

    # Pass the learned *tensor* positionally for inference
    reward_diff_new_user = lore_model(test_pair_chosen, test_pair_rejected, new_user_learned_weights)
    prob_chosen_preferred = 1 / (1 + torch.exp(-reward_diff_new_user))

    print(f"\nPrediction for New User on Test Pair:")
    print(f"  Reward Difference (r_chosen - r_rejected): {reward_diff_new_user.item():.4f}")
    print(f"  P(Chosen > Rejected): {prob_chosen_preferred.item():.4f}")

    # ***** Correction: Pass user_id 0 positionally *****
    reward_diff_seen_user = lore_model(test_pair_chosen, test_pair_rejected, 0) # Use positional argument
    prob_chosen_preferred_seen = 1 / (1 + torch.exp(-reward_diff_seen_user))

    print(f"\nPrediction for Seen User 0 on Test Pair:")
    print(f"  Reward Difference (r_chosen - r_rejected): {reward_diff_seen_user.item():.4f}")
    print(f"  P(Chosen > Rejected): {prob_chosen_preferred_seen.item():.4f}")